---
title: "A supplment material to understand Latent Dirichlet Allocation"
author: "Saki Kuzushima"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage{amsfonts}
   - \usepackage{amsmath}
   - \usepackage{graphicx}
output: pdf_document
---

\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}

The Latent Dirichlet Allocation model is very popular among the political scientists, and many people have summarized the model and the inference. However, understanding LDA requires some basis. This material aims to provide essential definitions and theorems to understand LDA and their variants. First section sketches the model and the infernece of LDA, and the second section describes some definitions and theorems essential to understand the model and the vaious inferences. 

# Part I

# 1. Latent Dirichlet Allocation model(s)

## 1.1  The original model

We firstly define the key terms used in the model. 
- Word: an element of a vocabulary. 
- Vocabulary: a set of $V$ unique words appear in the entire corpus. 
- Document: a sequence of $N$ words, denoted by $d = (w_1, w_2, ,..w_N)$
- Corpus: a set of $M$ documents, denoted by $D =(d_1, d_2...d_M)$
- Topic: a distribution of words. We assume that the number of topics is $K$. 

LDA is the following generating probabilistic model. 

- For each document, $d_j = (w_{j,1}, w_{j,2}, ,..w_{j,N}), j\in[1...M]$,
  + $\theta_j \sim Dirichlet(\alpha)$
  + For each word, $w_n, n \in [1...N]$
    + $z_{j,n} \sim Mutlnimoial(\theta_j)$
    * $w_{j,n} \sim Multinomial(\phi_{z_{j,n}})$


For instance, we have three topics on a news article: sports, politics, and technology ($K = 3$). $\theta_j$, which is the distribution of these topics over the document $d_j$ was drawn from the Dirichlet distribution. We assume that the sampled $\theta_j = [\frac{1}{3}, \frac{1}{4}, \frac{5}{12}]$, where the order of topics are [sports, politics, technology]. Substantively, this means that the document $d_j$ is a mixture of $1/3$ "sports", $1/4$ "poliitcs", and $5/12$ "technology". 

Next, we draw the topic of each word $z_{j,n}$ from $Multinomial(\theta)$. We assume this happens to be "politics." Then, we sample each word $w_{j,n}$ from $Multinomial(\phi_{z{j,n}})$. 

Assuming that the documents and the words are independent, $d_j \indep d_{j'}$ and $w_{j,n} \indep w_{j,n'}$, we can write the joint distribution of $W, Z, \Theta$ given $\alpha, \Phi$ in the following way.
\begin{align}
p(W, Z, \Theta|\alpha, \Phi) = \prod_{j=1}^M p(\theta_j|\alpha)\prod_{j=1}^K p(z_{j,n}|\theta_j)p(w_{j,n}|\phi_{z_{j,n}})
\end{align}




## 1.2 Smoothed LDA

Alternatively, we can assign a prior to $\phi$. This model is called smoothed LDA.

- For each topic, $\phi_i \sim Dirichlet(\beta), i \in [1,...,K]$
- For each document, $d_j = (w_{j,1}, w_{j,2}, ,..w_{j,N}), j\in[1...M]$,
  + $\theta_j \sim Dirichlet(\alpha)$
  + For each word, $w_n, n \in [1...N]$
    + $z_{j,n} \sim Mutlnimoial(\theta_j)$
    * $w_{j,n} \sim Multinomial(\phi_{z_{j,n}})$

![LDA (Source: Blei et al. 2003)](lda_blei2003.png){width=50%}



![Smoothed LDA (Source: Blei et al. 2003)](smoothlda_blei2003.png){width=50%}



In this model, we can write the joint distribution of $W, Z, \Theta$ given $\alpha, \beta$. 
\begin{align}
p(W, Z, \Theta|\alpha, \beta) = \prod_{i=1}^K p(\phi_j|\beta)\prod_{j=1}^M p(\theta_j|\alpha)\prod_{j=1}^K p(z_{j,n}|\theta_j)p(w_{j,n}|\phi_{z_{j,n}})
\end{align}

\newpage

## 1.3 Supervised LDA

LDA does not guarantee that it returns the topics we are substantively interested in. For instance, we might be interested in the sentiment of movie review, but the topics generated by the LDA might represents categories of movies (e.g. comedy, drama, romance etc...). Supervised LDA aims to ameliorate this problem, by incorporating humans' input. (Blei and McAuliffe 2008). The model is the following. 

- For each document, $d_j = (w_{j,1}, w_{j,2}, ,..w_{j,N}), j\in[1...M]$,
  + $\theta_j \sim Dirichlet(\alpha)$
  + For each word, $w_n, n \in [1...N]$
    * $z_{j,n} \sim Mutlnimoial(\theta_j)$
    * $w_{j,n} \sim Multinomial(\phi_{z_{j,n}})$
  + $y|z_{1:N}, \eta, \sigma^2 \sim N(\eta^{\intercal}\bar{z}, \sigma^2)$
  
![supervised LDA (Source: Blei and McAuliffe 2008)](slda_blei2008.png){width=50%}

where $\bar{z} = \frac{1}{N} \sum_{n=1}^N z_n$. The last line is the change from the original LDA. It is a normal linear model, but we could change it to a generalizing linear model, particularly when $y$ is a categorical variable. The covariate is the average number of each topic of a parcitular document. 

## 1.4 Correlated Topipc Model (CTM)

One of the problems of LDA is that the topics are assumed to be nearly independent. However, topics are likely to be correlated. For instance, in the corpus of academic journal articles, a document with a topic about public opinion is more likely to contain a topic about election than a topic about Bayesian statistics. This problem arises from the Dirichlet prior to the topic-word parameter. CTM replaces Dirichlet to logit-normal distribution, and the covariance matrix of normal distribution can control the correlation across topics. The drawback of this model is the loss of conjugacy between Multinomial and Dirichlet. Blei and Lafferty (2007) uses varitaionl inference to approximate posterior distribution.  

![Correlated Topic Model (Source: Blei and Laffarty 2007)](ctm_blei2007.png){width=50%}

- $\eta_d |\mu, \Sigma \sim N(\mu, \Sigma)$
- For $n \in {1...N_d}$,
  + $Z_{d,n}|\eta_d \sim Multinomial(f(\eta_d))$
  + $W_{d,n} | z_{d,n, \beta_{1:K}} \sim Multinomial(\beta_{z_{d,n}})$
  
where $f(\eta) = \frac{\exp(\eta)}{\sum_i \exp(\eta_i)}$.



# 2. Inference

## 2.1 Variational Inference (original method by Blei et al.)

The original inference method uses variational inference. Variational inference essentially approximate the intractable posterior with another distribution that varies across some parameters, and tune the parameters to minimize the difference between the posterior and the approximate distribution. The difference of the posterior and the approximate distributions are measured by the Kullback-Leibler divergence.

For the two pdf $p$ and $q$, which are defined on the same probability space, the Kullback-Leibler divergence between P and Q is 
$$
KL(g || p) = -\E_g\Big(\log(\frac{p(\theta|y)}{g(\theta)})  \Big) = - \int  \log(\frac{p(\theta|y)}{g(\theta)}) g(\theta) d\theta
$$

for more information about KL divergence, [this blog](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained) is helpful



# Part II

# 1. The Dirichlet is a conjugate prior of the multinomial

The Dirichelt distribution and the multinomial distribution are the key distributions for the latent Dirichlet allocation. First we review the key features of the two distributions, and confirm that the Dirichlet distribution is a conjugate prior to the multinomial distribution. 

The multinomial distribution has the following pmf. If
\begin{align*}
  X \sim Multinomial(\theta, n)
\end{align*}
where $X = (X_1, ...., X_n)$, $\theta = (\theta_1, ...\theta_k)$. Then, 

\begin{align}
  p(x|\theta, n) = \frac{n!}{\prod_{i=1}^K x_i!} \prod_{i=1}^K \theta_i^{x_i} 
\end{align}
where $x = (x_1, ...., x_n)$, and $\sum_{i=1}^K x_i = n$.

The Dirichlet distribution has the following pdf. If 
\begin{align*}
\theta \sim Dirichlet (\alpha)
\end{align*}

Then, 

\begin{align}
p (\theta|\alpha) = \frac{1}{B(\alpha)} \prod_{i=1}^K \theta_i^{\alpha_i -1}    
\end{align}

where $\sum_{i=1}^K \theta_i = 1$ and $\theta_i \geq 0, \forall i$. (i.e. $\theta$ belongs to the $K-1$ simplex.)

We can intuitively think how these distributions are related. We can think of the multinomial distribution as a result of $n$ consecutive rolls of a die, where $X = (X_1,...,X_6)$ is the number of each face showing up during the $n$ rolls. Then, the Dirichlet distribution is the distribution over the probability of each face showing up in each roll. (If a die is fair, then $\theta = (1/6, ...1/6)$). 

We can mathmatically show that the Dirichlet distribution is a conjugate prior of the multinomial distribution. The posterior of $\theta$ is

\begin{align*}
p(\theta|X, \alpha) \propto p(X|\theta, \alpha) p(\theta|\alpha)
\end{align*}

Then, the RHS is

\begin{align*}
  &p(X|\theta, \alpha) p(\theta|\alpha) \\
  &=  \frac{n!}{\prod_{i=1}^K x_i!} \prod_{i=1}^K \theta_i^{x_i} \frac{1}{B(\alpha)} \prod_{i=1}^K \theta_i^{\alpha_i -1} \\
  &\propto \prod_{i=1}^K  \theta_i^{x_i + \alpha_i -1}
\end{align*}

This is the kernel of the Dirichlet pdf. So we find that the posterior is also the Dirichlet distribution. By adjusting the constant term, we find that

\begin{align}
  p(\theta|X) = \frac{1}{B(\alpha + x)} \prod_{i=1}^K  \theta_i^{x_i + \alpha_i -1}
\end{align}

Therefore, the posterior distribution of $\theta$ is

\begin{align*}
  \theta^{post} \sim Dirichlet(\alpha + x)
\end{align*}

## 1.1 The relationship of the kernel of Dirichlet and Beta and Gamma functions

It is also helpful to see the integral of the kernel of Dirichlet is a Beta function, and therefore a ratio of gamma functions. This equation is useful in the derivation of the 
collapsed gibbs sampler. 

\begin{align*}
&p(\theta|\alpha) = \frac{1}{B(\alpha)}\prod_{i=1}^K\theta_i^{\alpha_i - 1} \\
&\iff 
1 = \int_\theta p(\theta|\alpha) d\theta = \int_\theta \frac{1}{B(\alpha)}\prod_{i=1}^K\theta_i^{\alpha_i - 1} d\theta\\
&\iff B(\alpha) = \int_\theta \prod_{i=1}^K\theta_i^{\alpha_i - 1} d\theta\\
&\iff \frac{\prod_{i=1}^K \Gamma(\alpha_i)}{\Gamma(\sum_{i=1}^K\alpha_i)} = \int_\theta \prod_{i=1}^K\theta_i^{\alpha_i - 1} d\theta
\end{align*}



# 2. The Dirichlet distribution belongs to the exponential family

The fact that the Dirichlet distribution belongs to the exponential family is useful. While the expression of the Dirichlet in terms of the exponential family is widely available, the derivation is not. The following sketches the conversion from the usual expression of the pdf of Dirichlet distribution to the form in terms of the exponential family.  

The pdf of the Dirichlet distribution is 
\begin{align}
  p(\theta|\alpha) = \frac{1}{B(\alpha)}\prod_{i=1}^K \theta_i^{\alpha_i - 1}
\end{align}
where $\theta = [\theta_1....\theta_K]^\intercal$, and $\sum_{i=1}^K \theta_i = 1$, $\theta_i \geq 0, \forall i \in \{1,2,...,K\}$.

The exponential family has the following general pdf
\begin{align}
  p(\theta|\eta) = h(\theta) \exp(\eta^\intercal t(\theta) - A(\eta))
\end{align}

where $t(\theta)$ is the sufficient statistic, $\eta$ is called the natural parameter, $A(\eta)$ is the log normalization factor, and $h(x)$ is the base measure.

It is known that the Dirichlet distribution belongs to the exponential family, and the parameters are

- The natural parameter: $\eta = \alpha = [\alpha_1...\alpha_K]^\intercal$
- The sufficient statistic: $t(\theta) = \log \theta = \log[\theta_1, ...\theta_K]^\intercal$.
- The log normalization factor: $A(\eta) = \sum_{i=1}^K \log \Gamma(\eta_i) - \log \Gamma(\sum_{i=1}^K \eta_i)$
- The base measure: $h(x) = \frac{1}{\prod_{i=1}^K} \theta_i$

The following sketches why the parameters are expressed in these ways. 

First, recall that $B(\alpha, \beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)}$). Then the pdf of the Dirichlet distribution is 
$$
\begin{aligned}
p(\theta|\alpha) &= \frac{1}{B(\alpha)} \prod_{i=1}^K \theta_i^{\alpha_i - 1} \\
&= \frac{\Gamma(\sum_{i=1}^K \alpha_i)}{\prod_{i=1}^K\Gamma(\alpha_i)}\cdot \frac{\prod_{i=1}^K \theta_i^{\alpha_i}}{\prod_{i=1}^K \theta_i}
\end{aligned}
$$
Taking the exponential and log, 
$$
\begin{aligned}
p(\theta|\alpha) 
&= \frac{\Gamma(\sum_{i=1}^K \alpha_i)}{\prod_{i=1}^K\Gamma(\alpha_i)}\cdot \frac{\prod_{i=1}^K \theta_i^{\alpha_i}}{\prod_{i=1}^K \theta_i} \\
&= \frac{1}{\prod_{i=1}^K \theta_i} \exp\Big[\log \frac{\Gamma(\sum_{i=1}^K \alpha_i)}{\prod_{i=1}^K\Gamma(\alpha_i)}\prod_{i=1}^K \theta_i^{\alpha_i}\Big] \\
&= \frac{1}{\prod_{i=1}^K \theta_i} \exp\Big[\log \Gamma(\sum_{i=1}^K \alpha_i) - \log\prod_{i=1}^K\Gamma(\alpha_i) + \log\prod_{i=1}^K \theta_i^{\alpha_i}\Big] \\
&=  \frac{1}{\prod_{i=1}^K \theta_i} \exp\Big[\log \Gamma(\sum_{i=1}^K \alpha_i) - \sum_{i=1}^K \log \Gamma(\alpha_i) + \sum_{i=1}^K \alpha_i \log \theta_i\Big] \\
&= \frac{1}{\prod_{i=1}^K \theta_i} \exp\Big[\alpha^\intercal \log \theta 
- \big\{\sum_{i=1}^K\log \Gamma(\alpha_i) -
\log \Gamma(\sum_{i=1}^K \alpha_i) \big\} \Big] \\
\end{aligned}
$$

Comparison of this expression with the general pdf of the exponential family yields the parameters listed above. 




# 3.  The expectation of the sufficient statistic of the exponential family is the derivative of the log normalizing factor 

Note that this is useful in the derivation of the original variational inference for LDA (in Blei et al 2003). 

Theorem:

\begin{align}
  \E_\theta[t(\theta)] = \frac{\partial}{\partial \eta} A(\eta) 
\end{align}

Proof:

Using the fact that the pdf must integrate to one, we first express $a(\eta)$ by the other parameters. 

$$
\begin{aligned}
  &1 = \int_\theta p(\theta|\eta) d\theta = \int_\theta h(\theta) \exp(\eta\ t(\theta) - A(\eta)) d\theta \\
  &\iff 1 = \frac{1}{\exp A(\eta)} \int_\theta h(\theta) \exp(\eta\ t(\theta)) d\theta \\
  &\iff \exp A(\eta) = \int_\theta h(\theta) \exp(\eta\ t(\theta)) d\theta \\
  &\iff A(\eta) = \log \int_\theta h(\theta) \exp(\eta\ t(\theta)) d\theta \\
\end{aligned}
$$
Let $g(\eta) = \frac{1}{\exp A(\eta)}$. Then this is also equivalent to 
\begin{align}
1 = g(\eta) \int_\theta h(\theta) \exp(\eta\ t(\theta)) d\theta 
\end{align}

Taking the derivative of Eq (3) with respect to $\eta$, 
$$
\begin{aligned}
0 &= \frac{\partial}{\partial \eta}\Big[ g(\eta)\int_\theta h(\theta) \exp(\eta\ t(\theta)) d\theta \Big]\\
&=   g'(\eta)\int_\theta h(\theta) \exp(\eta\ t(\theta)) d\theta +
g(\eta) \frac{\partial}{\partial \eta}\int_\theta h(\theta) \exp(\eta\ t(\theta)) d\theta \\
&= \frac{g'(\eta)}{g(\eta)} + g(\eta) \int_\theta h(\theta) \exp(\eta\ t(\theta)) t(\theta) d\theta \\
&= \frac{g'(\eta)}{g(\eta)} + \int_\theta g(\eta) h(\theta) \exp(\eta\ t(\theta)) t(\theta) d\theta \\
&= \frac{g'(\eta)}{g(\eta)} + \E_\theta[t(\theta)]  \\
\end{aligned}
$$

i.e. 
\begin{align}
  \E_\theta[t(\theta)] = -\frac{g'(\eta)}{g(\eta)} = -\frac{\partial}{\partial \eta}log(g(\eta))
\end{align}
because $\frac{\partial}{\partial \eta}log(g(\eta)) = \frac{1}{g(\eta)}\cdot\frac{\partial}{\partial \eta}g(\eta)$.

Because $g(\eta) = \frac{1}{\exp(A\eta))} = -\exp(A(\eta))$, Eq (4) is equivalent to
\begin{align}
  \E[t(\theta)] = -\frac{\partial}{\partial \eta}log(g(\eta)) =\frac{\partial}{\partial \eta} A(\eta)
\end{align}

# 4. Derivation steps for the collapsed Gibbs Sampler for the smoothed LDA 

This section describes the steps for the derivation of the collapsed Gibbs Sampler for the 
smoothed LDA. It aims to fill the gaps of the lecture slides by Shiraito for POLSCI798. 
The model is specified in Part I, smoothed LDA. (Notation has changed. Need to fix. )
Because the following equations involve many products,
let us assume that the product term,$\prod$, ends at the $\times$. 


## Joint posterior distribution

\begin{align}
  &p(Z, \theta, \eta|W, \alpha, \beta) \\
  &\propto \underbrace{p(\eta | \beta) p(\theta|\alpha)}_{\text{prior}} \times
  \underbrace{p(Z|\theta)p(W|Z, \eta)}_{\text{likelihood}} \\
  &= \prod_{k=1}^K p(\eta_k|\beta) \times \prod_{j=1}^J p(\theta_j|\alpha) \times
  \prod_{j=1}^J \prod_{i=1}^{N_j} p(Z_{ij}|\theta_j) \times
  \prod_{j=1}^J \prod_{i=1}^{N_j} p(W_{ij}|Z_{ij},\eta_j) \\
  &=  \prod_{k=1}^K p(\eta_k|\beta) \times \prod_{j=1}^J p(\theta_j|\alpha) 
   \prod_{i=1}^{N_j} p(Z_{ij}|\theta_j)  p(W_{ij}|Z_{ij},\eta_j) \\
\end{align}

## Joint posterior distribution after collapsing $\theta$ and $\eta$


\begin{align}
p(Z|W) &\propto \int_\theta \int_\eta p(Z, \theta, \eta|W)\ d\eta d\theta \\
  &= \int_\theta \int_\eta  \prod_{k=1}^K p(\eta_k|\beta) \times \prod_{j=1}^J p(\theta_j|\alpha) 
   \prod_{i=1}^{N_j} p(Z_{ij}|\theta_j)\ d\eta d\theta \\
  &= \int_\theta \prod_{j=1}^J p(\theta_j|\alpha) \prod_{i=1}^{N_j}p(Z_{ij}|\theta_j)\ d\theta\ \times
  \int_\eta \prod_{k=1}^K p(\eta_k|\beta) \times \prod_{j=1}^J \prod_{i=1}^{N_j} p(Z_j|\theta_j)p(W_{ij}|Z_j, \eta)\ d\eta
\end{align}

## Conditional posterior 

$$
\begin{aligned}
&p(Z_{i^*j^*} = k|Z_{-i^*j^*},W) \\
&\propto \int_{\theta_{j^*}} p(\theta_{j^*}|\alpha) \prod_{i=1}^{N_j} p(Z_{i^*j^*} = k, 
\theta_{j^*}) \prod_{i \neq i^*} p(Z_{ij}|\theta_{j^*})\ d\theta_{j^*} \\
&\quad \times \int_{\eta_k} p(\eta_k|\beta) \times p(W_{i^*j^*}|Z_{i^*j^*} = k,
\eta_k)  \times \prod_{ij \neq i^*j^*, Z_{ij} = k} p(W_ij|Z_{ij}=k, \eta_k)\ d\eta_k
\end{aligned}
$$

### NOTE: diagram of data generating process is helpful - insert here








# Reference
Blei, D.M., Ng, A.Y. and Jordan, M.I., 2003. Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), pp.993-1022.

Lafferty, J.D. and Blei, D.M., 2006. Correlated topic models. In Advances in neural information processing systems (pp. 147-154).

Mcauliffe, J.D. and Blei, D.M., 2008. Supervised topic models. In Advances in neural information processing systems (pp. 121-128).

[Geigle, C., 2016. Inference Methods for Latent Dirichlet Allocation.](http://times.cs.uiuc.edu/course/598f16/notes/lda-survey.pdf)

Gelman, A., Stern, H.S., Carlin, J.B., Dunson, D.B., Vehtari, A. and Rubin, D.B., 2013. Bayesian data analysis. Chapman and Hall/CRC.

Will Kurt. [A blog post about KL divergence](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)

Lecture Slides by Yuki Shiraito, POLSCI798, 2019. 






